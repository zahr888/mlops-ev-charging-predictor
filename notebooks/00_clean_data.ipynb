{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90a0ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03d37d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>garage_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>shared_id</th>\n",
       "      <th>start_plugin</th>\n",
       "      <th>start_plugin_hour</th>\n",
       "      <th>end_plugout</th>\n",
       "      <th>end_plugout_hour</th>\n",
       "      <th>el_kwh</th>\n",
       "      <th>duration_hours</th>\n",
       "      <th>month_plugin</th>\n",
       "      <th>weekdays_plugin</th>\n",
       "      <th>plugin_category</th>\n",
       "      <th>duration_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AdO3</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-21 10:20:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-12-21 10:23:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Friday</td>\n",
       "      <td>late morning (9-12)</td>\n",
       "      <td>Less than 3 hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AdO3</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-21 10:24:00</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-12-21 10:32:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Friday</td>\n",
       "      <td>late morning (9-12)</td>\n",
       "      <td>Less than 3 hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AdO3</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-21 11:33:00</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-12-21 19:46:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.87</td>\n",
       "      <td>8.216389</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Friday</td>\n",
       "      <td>late morning (9-12)</td>\n",
       "      <td>Between 6 and 9  hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AdO3</td>\n",
       "      <td>AdO3-2</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-22 16:15:00</td>\n",
       "      <td>16</td>\n",
       "      <td>2018-12-23 16:40:00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.56</td>\n",
       "      <td>24.419722</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>late afternoon (15-18)</td>\n",
       "      <td>More than 18 hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AdO3</td>\n",
       "      <td>AdO3-2</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-24 22:03:00</td>\n",
       "      <td>22</td>\n",
       "      <td>2018-12-24 23:02:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.970556</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Monday</td>\n",
       "      <td>late evening (21-midnight)</td>\n",
       "      <td>Less than 3 hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id garage_id user_id user_type shared_id        start_plugin  \\\n",
       "0           1      AdO3  AdO3-4   Private      None 2018-12-21 10:20:00   \n",
       "1           2      AdO3  AdO3-4   Private      None 2018-12-21 10:24:00   \n",
       "2           3      AdO3  AdO3-4   Private      None 2018-12-21 11:33:00   \n",
       "3           4      AdO3  AdO3-2   Private      None 2018-12-22 16:15:00   \n",
       "4           5      AdO3  AdO3-2   Private      None 2018-12-24 22:03:00   \n",
       "\n",
       "   start_plugin_hour         end_plugout  end_plugout_hour  el_kwh  \\\n",
       "0                 10 2018-12-21 10:23:00              10.0    0.30   \n",
       "1                 10 2018-12-21 10:32:00              10.0    0.87   \n",
       "2                 11 2018-12-21 19:46:00              19.0   29.87   \n",
       "3                 16 2018-12-23 16:40:00              16.0   15.56   \n",
       "4                 22 2018-12-24 23:02:00              23.0    3.62   \n",
       "\n",
       "   duration_hours month_plugin weekdays_plugin             plugin_category  \\\n",
       "0        0.050000          Dec          Friday         late morning (9-12)   \n",
       "1        0.136667          Dec          Friday         late morning (9-12)   \n",
       "2        8.216389          Dec          Friday         late morning (9-12)   \n",
       "3       24.419722          Dec        Saturday      late afternoon (15-18)   \n",
       "4        0.970556          Dec          Monday  late evening (21-midnight)   \n",
       "\n",
       "        duration_category  \n",
       "0       Less than 3 hours  \n",
       "1       Less than 3 hours  \n",
       "2  Between 6 and 9  hours  \n",
       "3      More than 18 hours  \n",
       "4       Less than 3 hours  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling the parquets from s3 and loading into pandas dataframes\n",
    "\n",
    "storage_options = {\n",
    "    \"client_kwargs\": {\"endpoint_url\": \"http://localhost:4566\"},\n",
    "    \"key\": \"test\",\n",
    "    \"secret\": \"test\"\n",
    "}\n",
    "df = pd.read_parquet('s3://ev-data/raw/', \n",
    "                           storage_options=storage_options,\n",
    "                           engine='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdeda877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['session_id', 'garage_id', 'user_id', 'user_type', 'shared_id',\n",
       "       'start_plugin', 'start_plugin_hour', 'end_plugout', 'end_plugout_hour',\n",
       "       'el_kwh', 'duration_hours', 'month_plugin', 'weekdays_plugin',\n",
       "       'plugin_category', 'duration_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standardizing column names\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "299452a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting el_kwh and duration_hours to numeric, coercing errors to NaN\n",
    "df['el_kwh'] = pd.to_numeric(df['el_kwh'], errors='coerce')\n",
    "df['duration_hours'] = pd.to_numeric(df['duration_hours'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452a376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6878 entries, 0 to 6877\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   session_id         6878 non-null   int64         \n",
      " 1   garage_id          6878 non-null   object        \n",
      " 2   user_id            6878 non-null   object        \n",
      " 3   user_type          6878 non-null   object        \n",
      " 4   shared_id          1412 non-null   object        \n",
      " 5   start_plugin       6878 non-null   datetime64[ns]\n",
      " 6   start_plugin_hour  6878 non-null   int64         \n",
      " 7   end_plugout        6844 non-null   datetime64[ns]\n",
      " 8   end_plugout_hour   6844 non-null   float64       \n",
      " 9   el_kwh             6878 non-null   float64       \n",
      " 10  duration_hours     6844 non-null   float64       \n",
      " 11  month_plugin       6878 non-null   object        \n",
      " 12  weekdays_plugin    6878 non-null   object        \n",
      " 13  plugin_category    6878 non-null   object        \n",
      " 14  duration_category  6844 non-null   object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(2), object(8)\n",
      "memory usage: 806.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ed0d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id              0\n",
       "garage_id               0\n",
       "user_id                 0\n",
       "user_type               0\n",
       "shared_id            5466\n",
       "start_plugin            0\n",
       "start_plugin_hour       0\n",
       "end_plugout            34\n",
       "end_plugout_hour       34\n",
       "el_kwh                  0\n",
       "duration_hours         34\n",
       "month_plugin            0\n",
       "weekdays_plugin         0\n",
       "plugin_category         0\n",
       "duration_category      34\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b81f3e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with both end_plugout and duration_hours NULL: 34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>garage_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>shared_id</th>\n",
       "      <th>start_plugin</th>\n",
       "      <th>start_plugin_hour</th>\n",
       "      <th>end_plugout</th>\n",
       "      <th>end_plugout_hour</th>\n",
       "      <th>el_kwh</th>\n",
       "      <th>duration_hours</th>\n",
       "      <th>month_plugin</th>\n",
       "      <th>weekdays_plugin</th>\n",
       "      <th>plugin_category</th>\n",
       "      <th>duration_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>5173</td>\n",
       "      <td>SR2</td>\n",
       "      <td>SR2-3</td>\n",
       "      <td>Private</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-12-19 16:23:00</td>\n",
       "      <td>16</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>late afternoon (15-18)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id garage_id user_id user_type shared_id        start_plugin  \\\n",
       "5172        5173       SR2   SR2-3   Private      None 2019-12-19 16:23:00   \n",
       "\n",
       "      start_plugin_hour end_plugout  end_plugout_hour  el_kwh  duration_hours  \\\n",
       "5172                 16         NaT               NaN   24.36             NaN   \n",
       "\n",
       "     month_plugin weekdays_plugin         plugin_category duration_category  \n",
       "5172          Dec        Thursday  late afternoon (15-18)              None  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fixing missing values in 'end_plugout' and 'duration_hours' by assigning correct values\n",
    "missing_both = df[df['end_plugout'].isna() & df['duration_hours'].isna() & df['duration_category'].isna()]\n",
    "print(\"Rows with both end_plugout and duration_hours NULL:\", len(missing_both))\n",
    "missing_both.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b893d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average charging rate: 1.52 kWh/hour\n",
      "Found 0 missing sessions\n",
      "Fixed 0 missing sessions\n",
      "session_id              0\n",
      "garage_id               0\n",
      "user_id                 0\n",
      "user_type               0\n",
      "shared_id            5466\n",
      "start_plugin            0\n",
      "start_plugin_hour       0\n",
      "end_plugout             0\n",
      "end_plugout_hour        0\n",
      "el_kwh                  0\n",
      "duration_hours          0\n",
      "month_plugin            0\n",
      "weekdays_plugin         0\n",
      "plugin_category         0\n",
      "duration_category       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: calculate average charging rate\n",
    "complete_sessions = df.dropna(subset=['end_plugout', 'duration_hours']).copy()\n",
    "complete_sessions['charging_rate'] = complete_sessions['el_kwh'] / complete_sessions['duration_hours']\n",
    "avg_charging_rate = complete_sessions['charging_rate'].median()\n",
    "\n",
    "print(f\"Average charging rate: {avg_charging_rate:.2f} kWh/hour\")\n",
    "\n",
    "# Step 2: Get indices of missing rows before fixing them\n",
    "missing_indices = df[df['end_plugout'].isna() & df['duration_hours'].isna()].index\n",
    "print(f\"Found {len(missing_indices)} missing sessions\")\n",
    "\n",
    "# Step 3: Estimate missing durations using average rate\n",
    "df.loc[missing_indices, 'duration_hours'] = df.loc[missing_indices, 'el_kwh'] / avg_charging_rate\n",
    "\n",
    "# Step 4: Calculate end_plugout from estimated duration\n",
    "df.loc[missing_indices, 'end_plugout'] = df.loc[missing_indices, 'start_plugin'] + pd.to_timedelta(df.loc[missing_indices, 'duration_hours'], unit='h')\n",
    "\n",
    "# Step 5: Update end_plugout_hour\n",
    "df.loc[missing_indices, 'end_plugout_hour'] = df.loc[missing_indices, 'end_plugout'].dt.hour\n",
    "\n",
    "# Step 6: Update duration_category based on estimated duration\n",
    "for idx in missing_indices:\n",
    "    duration = df.loc[idx, 'duration_hours']\n",
    "    if duration > 18:\n",
    "        df.loc[idx, 'duration_category'] = \"More than 18 hours\"\n",
    "    elif duration > 15:\n",
    "        df.loc[idx, 'duration_category'] = \"Between 15 and 18 hours\"\n",
    "    elif duration > 12:\n",
    "        df.loc[idx, 'duration_category'] = \"Between 12 and 15 hours\"\n",
    "    elif duration > 9:\n",
    "        df.loc[idx, 'duration_category'] = \"Between 9 and 12 hours\"\n",
    "    elif duration > 6:\n",
    "        df.loc[idx, 'duration_category'] = \"Between 6 and 9 hours\"\n",
    "    elif duration > 3:\n",
    "        df.loc[idx, 'duration_category'] = \"Between 3 and 6 hours\"\n",
    "    else:\n",
    "        df.loc[idx, 'duration_category'] = \"Less than 3 hours\"\n",
    "\n",
    "print(f\"Fixed {len(missing_indices)} missing sessions\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b13d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Less than 3 hours\n",
       "1            Less than 3 hours\n",
       "2       Between 6 and 9  hours\n",
       "3           More than 18 hours\n",
       "4            Less than 3 hours\n",
       "                 ...          \n",
       "6873     Between 3 and 6 hours\n",
       "6874     Between 3 and 6 hours\n",
       "6875         Less than 3 hours\n",
       "6876         Less than 3 hours\n",
       "6877     Between 3 and 6 hours\n",
       "Name: duration_category, Length: 6878, dtype: category\n",
       "Categories (7, object): ['Between 12 and 15 hours', 'Between 15 and 18 hours', 'Between 3 and 6 hours', 'Between 6 and 9  hours', 'Between 9 and 12 hours', 'Less than 3 hours', 'More than 18 hours']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f88949ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix duration mismatches\n",
    "df['charging_duration'] = (df['end_plugout'] - df['start_plugin']).dt.total_seconds() / 3600\n",
    "duration_diff = abs(df['duration_hours'] - df['charging_duration'])\n",
    "df.loc[duration_diff > 0.1, 'duration_hours'] = df['charging_duration']\n",
    "df = df.drop('charging_duration', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0f13036",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m category_columns:\n\u001b[32m      5\u001b[39m     df[col] = df[col].astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mend_plugout_hour\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mend_plugout_hour\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mint64\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.dtypes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:101\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.ensure_string_array(\n\u001b[32m     97\u001b[39m         arr, skipna=skipna, convert_na_value=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     ).reshape(shape)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m np.issubdtype(arr.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_np_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "category_columns = [\n",
    "    'user_type', 'shared_id', 'month_plugin', 'weekdays_plugin', 'plugin_category', 'duration_category'\n",
    "]\n",
    "for col in category_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "df['end_plugout_hour'] = df['end_plugout_hour'].astype('int64')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying start_plugin_hour\n",
    "df['hour'] = df['start_plugin'].dt.hour\n",
    "\n",
    "\n",
    "df['start_plugin_hour'] = np.where(\n",
    "    df['start_plugin_hour'] != df['hour'],\n",
    "    df['hour'],                             \n",
    "    df['start_plugin_hour']                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches left: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mismatches left:\", (df['start_plugin_hour'] != df['hour']).sum())\n",
    "df=df.drop(columns='hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating data on hourly basis\n",
    "df['hour'] = df['start_plugin'].dt.floor('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c16880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6844, 16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68875b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0789e6b",
   "metadata": {},
   "source": [
    "# Saving the df into a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22636960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved locally: C:/Users/GIGABYTE/Documents/ml/mlops/data/processed/cleaned_ev_data.parquet\n",
      "✅ Cleaned data uploaded to S3: s3://ev-data/processed/cleaned_ev_data.parquet\n",
      "Files in S3 processed folder:\n",
      "   - processed/cleaned_ev_data.parquet\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "local_processed_dir = \"C:/Users/GIGABYTE/Documents/ml/mlops/data/processed\"\n",
    "local_file_path = f\"{local_processed_dir}/cleaned_ev_data.parquet\"\n",
    "s3_bucket = \"ev-data\"\n",
    "s3_key = \"processed/cleaned_ev_data.parquet\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "Path(local_processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save locally\n",
    "df.to_parquet(local_file_path, \n",
    "                     engine='pyarrow', \n",
    "                     compression='snappy',\n",
    "                     index=False)\n",
    "print(f\"✅ Cleaned data saved locally: {local_file_path}\")\n",
    "\n",
    "# Uploading to S3\n",
    "s3 = boto3.client('s3', \n",
    "                 endpoint_url=\"http://localhost:4566\",\n",
    "                 aws_access_key_id=\"test\", \n",
    "                 aws_secret_access_key=\"test\")\n",
    "\n",
    "s3.upload_file(local_file_path, s3_bucket, s3_key)\n",
    "print(f\"✅ Cleaned data uploaded to S3: s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "# Verify upload\n",
    "response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=\"processed/\")\n",
    "if 'Contents' in response:\n",
    "    print(\"Files in S3 processed folder:\")\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"   - {obj['Key']}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cb6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ef840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport boto3\\nimport os\\n\\n# LocalStack endpoint\\nLOCALSTACK_ENDPOINT = \"http://localhost:4566\"\\ns3 = boto3.client(\\'s3\\', endpoint_url=LOCALSTACK_ENDPOINT,\\n                  aws_access_key_id=\"test\", aws_secret_access_key=\"test\")\\n\\n# create bucket if not exists (LocalStack ignores region)\\ntry:\\n    s3.create_bucket(Bucket=\"ev-data\")\\nexcept Exception as e:\\n    print(\"create_bucket:\", e)\\n\\n# if you saved a partitioned folder, upload recursively:\\nimport pathlib\\nfolder = pathlib.Path(\\'../data/processed/raw_parquet/trondheim_sessions_v1_partitioned\\')\\nfor p in folder.rglob(\\'*.parquet\\'):\\n    # compute key relative to folder\\n    relative = p.relative_to(folder)\\n    key = f\\'raw/trondheim_partitioned/{relative.as_posix()}\\'\\n    s3.upload_file(str(p), \"ev-data\", key)\\n    print(\"Uploaded:\", key)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uploading the partitioned data from local to s3\n",
    "'''\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# LocalStack endpoint\n",
    "LOCALSTACK_ENDPOINT = \"http://localhost:4566\"\n",
    "s3 = boto3.client('s3', endpoint_url=LOCALSTACK_ENDPOINT,\n",
    "                  aws_access_key_id=\"test\", aws_secret_access_key=\"test\")\n",
    "\n",
    "# create bucket if not exists (LocalStack ignores region)\n",
    "try:\n",
    "    s3.create_bucket(Bucket=\"ev-data\")\n",
    "except Exception as e:\n",
    "    print(\"create_bucket:\", e)\n",
    "\n",
    "# if you saved a partitioned folder, upload recursively:\n",
    "import pathlib\n",
    "folder = pathlib.Path('../data/processed/raw_parquet/trondheim_sessions_v1_partitioned')\n",
    "for p in folder.rglob('*.parquet'):\n",
    "    # compute key relative to folder\n",
    "    relative = p.relative_to(folder)\n",
    "    key = f'raw/trondheim_partitioned/{relative.as_posix()}'\n",
    "    s3.upload_file(str(p), \"ev-data\", key)\n",
    "    print(\"Uploaded:\", key)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413d43b",
   "metadata": {},
   "source": [
    "# Verifying the files existance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cbfcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchBucket",
     "evalue": "An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchBucket\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# List all objects recursively in the bucket\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43ms3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_objects_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mev-data-clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPrefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mev-clean/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print each file\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m response.get(\u001b[33m'\u001b[39m\u001b[33mContents\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\botocore\\client.py:602\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    599\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    600\u001b[39m     )\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\botocore\\context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GIGABYTE\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\botocore\\client.py:1078\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1074\u001b[39m     error_code = request_context.get(\n\u001b[32m   1075\u001b[39m         \u001b[33m'\u001b[39m\u001b[33merror_code_override\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1076\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1077\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1080\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mNoSuchBucket\u001b[39m: An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "# List all objects recursively in the bucket\n",
    "response = s3.list_objects_v2(Bucket=\"ev-data-clean\", Prefix=\"ev-clean/\")\n",
    "\n",
    "# Print each file\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f\"Key: {obj['Key']}, Size: {obj['Size']} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03bfa9",
   "metadata": {},
   "source": [
    "# Reading from the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311b296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndownload_path = \\'../data/downloaded/ev_downloaded.parquet\\'\\ns3.download_file(\"ev-data-clean\", \"ev-clean/year=2020/month=1/39b0d0debd45456488561c24e7466d83-0.parquet\", download_path)\\ndf2 = pd.read_parquet(download_path, engine=\\'pyarrow\\')\\ndf2.head()\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading then reading the downloaded file\n",
    "'''\n",
    "download_path = '../data/downloaded/ev_downloaded.parquet'\n",
    "s3.download_file(\"ev-data-clean\", \"ev-clean/year=2020/month=1/39b0d0debd45456488561c24e7466d83-0.parquet\", download_path)\n",
    "df2 = pd.read_parquet(download_path, engine='pyarrow')\n",
    "df2.head()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
